{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a45482e",
   "metadata": {},
   "source": [
    "# 3. Xception\n",
    "Our third attempt will be to use a pretrained model Xception. This model is bigger than MobileNetV2, but it should be also trainable in reasonable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3d32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11553b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda vesion: 11.2\n",
      "Cudnn vesion: 8\n"
     ]
    }
   ],
   "source": [
    "sys_details = tf.sysconfig.get_build_info()\n",
    "cuda_version = sys_details[\"cuda_version\"]\n",
    "print('Cuda vesion:', cuda_version)\n",
    "cudnn_version = sys_details[\"cudnn_version\"]  \n",
    "print('Cudnn vesion:', cudnn_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087f576",
   "metadata": {},
   "source": [
    "## Preparation and model creation\n",
    "Uncomment the configuration that you would like to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a1fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_dir = 'resized128/train'\n",
    "_test_dir = 'resized128/test'\n",
    "_img_width, _img_height = 128,128 \n",
    "_batch_size = 64\n",
    "_weights_name = 'maskedfaces128_xception.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e121a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _train_dir = 'resized224/train'\n",
    "# _test_dir = 'resized224/test'\n",
    "# _img_width, _img_height = 224,224 \n",
    "# _batch_size = 64\n",
    "# _weights_name = 'maskedfaces224_xception.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ecbf17",
   "metadata": {},
   "source": [
    "Note, that we using a preprocessing function argument, that contains preprocess_input function for Xception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d75dc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96658 images belonging to 2 classes.\n",
      "Found 17057 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.15) # set validation split\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    _train_dir,\n",
    "    target_size=(_img_height, _img_width),\n",
    "    batch_size=_batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training') # set as training data\n",
    "\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    _train_dir, # same directory as training data\n",
    "    target_size=(_img_height, _img_width),\n",
    "    batch_size=_batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e53de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20067 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True) \n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    _test_dir,\n",
    "    target_size=(_img_height, _img_width),\n",
    "    batch_size=_batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cfbeea",
   "metadata": {},
   "source": [
    "In this model we are adding a Xception model pretrained on the imagenet dataset. As didn't allow learning in previous model (and it didn't help), we will allow learning of all layers. After the Xception model, we will add one 2048 dense layer with relu activation function followed by a dropout layer. As before we add average pooling layer and after the Xception and in the end ouput dense layer. We are using Adam optimizer with 0.001 learning rate and accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "159b5168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 16:00:21.325331: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-29 16:00:21.804160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9705 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:0c:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " xception (Functional)       (None, 4, 4, 2048)        20861480  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,059,881\n",
      "Trainable params: 25,005,353\n",
      "Non-trainable params: 54,528\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Xception(include_top = False, weights=\"imagenet\", input_shape=(_img_width, _img_height, 3)))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(2048, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001),  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0213d99",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "We have also added a checkpoint save weights with the best validation loss. You can also [skip](#skip) the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0555346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=_weights_name, verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78c527",
   "metadata": {},
   "source": [
    "We will train the model for 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e13a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 16:00:33.313383: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1511/1511 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9896\n",
      "Epoch 00001: val_loss improved from inf to 0.00242, saving model to maskedfaces128_xception.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rbelanec/Documents/ml-project/ml/lib/python3.8/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1511/1511 [==============================] - 416s 271ms/step - loss: 0.0293 - accuracy: 0.9896 - val_loss: 0.0024 - val_accuracy: 0.9992\n",
      "Epoch 2/15\n",
      "1511/1511 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 00002: val_loss did not improve from 0.00242\n",
      "1511/1511 [==============================] - 396s 262ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.0164 - val_accuracy: 0.9960\n",
      "Epoch 3/15\n",
      "1511/1511 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9990\n",
      "Epoch 00003: val_loss improved from 0.00242 to 0.00217, saving model to maskedfaces128_xception.h5\n",
      "1511/1511 [==============================] - 399s 264ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.0022 - val_accuracy: 0.9993\n",
      "Epoch 4/15\n",
      " 616/1511 [===========>..................] - ETA: 3:22 - loss: 0.0025 - accuracy: 0.9992"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_generator, validation_data = valid_generator, epochs=15, callbacks=[checkpointer], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9581b027",
   "metadata": {},
   "source": [
    "Now we will plot the training history and evaluate our model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c71ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist.history['loss'], label='training loss')\n",
    "plt.plot(hist.history['val_loss'], label='validation loss')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist.history['accuracy'], label='train accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], label='validation accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f692fa",
   "metadata": {},
   "source": [
    "<a id='skip'></a> If you want to skip the training, just uncomment the first or the second cell under this text. But don't forget to recompile the model with desired (128/224) configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605dcea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget\n",
    "# _weights_name = 'maskedfaces128_xception.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486dfced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget\n",
    "# _weights_name = 'maskedfaces224_xception.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = model.evaluate(train_generator)\n",
    "print(\"\\n\\ntrain loss: {} | train acc: {}\\n\".format(train_score[0], train_score[1]))\n",
    "\n",
    "test_score = model.evaluate(test_generator)\n",
    "print(\"\\n\\ntest loss: {} | test acc: {}\".format(test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb229c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = model.predict(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(model_predictions > 0.5).astype(int).T[0]\n",
    "labels = np.array(test_generator.classes)\n",
    "\n",
    "f1 = f1_score(labels, preds)\n",
    "print('F1 score: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47209ab5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Our model with pretrained Xception model with disabled learning ended up with much better accuracy. The F1 score is still poor. We will continue in next notebook (TODO link), with our final custom network. The training of this model was around 6 minutes per epoch on 128x128 images and around 20 minutes per epoch on 224x224 images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee51925",
   "metadata": {},
   "source": [
    "### 224x224 images\n",
    "Our Xception model with 224x224 images ended up with results:\n",
    "- train loss: 0.0004\n",
    "- train accuracy: 0.99\n",
    "- test loss: 0.0002\n",
    "- test accuracy: 0.99\n",
    "- F1 score: 0.51"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331024de",
   "metadata": {},
   "source": [
    "### 128x128 images\n",
    "Our Xception model with 128x128 images ended up with results:\n",
    "- train loss: \n",
    "- train accuracy: \n",
    "- test loss: \n",
    "- test accuracy: \n",
    "- F1 score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40afd653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
